{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "## Postprocessing\n",
    "\n",
    "### Setup combined table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engines = [\"llamafile\"]\n",
    "models = [\"mistral\", \"dolphin-2.6-mistral\", \"dolphin-2.6-phi-2\", \"mixtral\", \"llama3\", \"phi-3\"]\n",
    "templates = [\"LIT\", \"controlled\", \"controlled_md\", \"via_description\", \"via_description_1_shot\"]\n",
    "datasets = [\"codenet\", \"avatar\", \"evalplus\", \"basicbench\", \"bithacks\"]\n",
    "\n",
    "pp_steps = ['MARKDOWN_CODEBLOCKS', 'NO_MARKDOWN', 'MISSING_MD_START', 'MISSING_MD_END', 'CODE_HEURISTIC', 'NATURAL_TEXT', 'ESCAPED_UNDERSCORES', 'MD_IN_CODE']\n",
    "pp_errors = ['NESTED', 'CODE_FENCE_IN_CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def list_json_files(directory: str | Path):\n",
    "    json_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".json\"):\n",
    "                json_file = os.path.join(root, filename)\n",
    "                json_files.append(json_file)\n",
    "    return json_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from codetransbench.utils.config import load_config\n",
    "path_to_config = \"../codetransbenchmark/config/config.yaml\"\n",
    "if path_to_config:\n",
    "    config = load_config(path_to_config)\n",
    "else:\n",
    "    config = load_config()\n",
    "\n",
    "pp_reports_dir = config.postprocessing_reports_dir\n",
    "\n",
    "def build_combined_postprocessing_table(pp_reports_dir: str | Path):\n",
    "    report_files = list_json_files(pp_reports_dir)\n",
    "    # print(report_files)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for report_f in report_files:\n",
    "        # postprocessing_reports/engine_model_template_attempt/jsonfilename\n",
    "        run_id = report_f.split(os.path.sep)[-2]\n",
    "        run_info = run_id.split(\"_\")\n",
    "        if len(run_info) == 3:\n",
    "            continue\n",
    "        engine = run_info[0]\n",
    "        model = run_info[1]\n",
    "        template = \"_\".join(run_info[2:-1])\n",
    "        attempt = int(run_info[-1])\n",
    "        # directory/pp_report_dataset.json\n",
    "        dataset = os.path.splitext(os.path.basename(report_f))[0].split(\"_\")[-1]\n",
    "\n",
    "        # print(engine, model, template, attempt, dataset) \n",
    "\n",
    "        with open(report_f, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        pp_steps_statistic = dict()\n",
    "        pp_errors_statistic = dict()\n",
    "\n",
    "        data_list = []\n",
    "        for file in data:\n",
    "            file_info = {}\n",
    "            file_info[\"file_id\"] = file\n",
    "            # file_info[\"report\"] = data[file]\n",
    "\n",
    "            info_list = file.split(\"_\")\n",
    "            file_info[\"source_pl\"] = info_list[1]\n",
    "            file_info[\"target_pl\"] = info_list[2]\n",
    "            file_info[\"filename\"] = \"_\".join(info_list[3:])\n",
    "\n",
    "            for step in data[file]['pp_steps']:\n",
    "                file_info[step] = 1\n",
    "                if step not in pp_steps_statistic.keys():\n",
    "                    pp_steps_statistic[step] = 1\n",
    "                else:\n",
    "                    pp_steps_statistic[step] += 1\n",
    "\n",
    "                # if step in [\"NATURAL_TEXT\", \"NO_MARKDOWN\", \"CODE_HEURISTIC\"]:\n",
    "                #     print(step, file)\n",
    "        \n",
    "            for error in data[file]['pp_errors']:\n",
    "                file_info[error] = 1\n",
    "                if error not in pp_errors_statistic.keys():\n",
    "                    pp_errors_statistic[error] = 1\n",
    "                else:\n",
    "                    pp_errors_statistic[error] += 1\n",
    "                # print(error, file)\n",
    "\n",
    "            data_list.append(file_info)\n",
    "\n",
    "        df = pd.json_normalize(data_list)\n",
    "        df[\"engine\"] = engine\n",
    "        df[\"model\"] = model\n",
    "        df[\"template\"] = template\n",
    "        df[\"attempt\"] = attempt\n",
    "        df[\"dataset\"] = dataset\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    combined_table = pd.concat(df_list)\n",
    "    combined_table.reset_index(drop=True, inplace=True)\n",
    "    return combined_table\n",
    "\n",
    "combined_table = build_combined_postprocessing_table(pp_reports_dir)\n",
    "ex = combined_table.sample(10)\n",
    "ex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex[[\"model\"] + pp_steps + pp_errors + [\"filename\"]].groupby([\"model\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_table(grouping_features: str | list[str], filter_query: str=None) -> pd.DataFrame:\n",
    "    combined_table = build_combined_postprocessing_table(pp_reports_dir)\n",
    "    if filter_query:\n",
    "        combined_table = combined_table.query(filter_query)\n",
    "    if not isinstance(grouping_features, list):\n",
    "        grouping_features = [grouping_features]\n",
    "    count_table = combined_table[grouping_features + pp_steps + pp_errors + [\"filename\"]].groupby(grouping_features).count()\n",
    "    total = count_table.sum()\n",
    "    if len(grouping_features) == 1:\n",
    "        total_index = \"Total\"\n",
    "    else:\n",
    "        total_index = tuple([\"Total\"] + [\"-\"] * (len(grouping_features) - 1))\n",
    "    count_table.loc[total_index, count_table.columns] = total\n",
    "    count_table.rename({\"filename\": \"Total Files\"}, axis=\"columns\", inplace=True)\n",
    "    count_table = count_table.convert_dtypes()\n",
    "    return count_table\n",
    "\n",
    "def create_percentage_table(count_table: pd.DataFrame)-> pd.DataFrame:\n",
    "    percentage_table = count_table[pp_steps + pp_errors].div(count_table[\"Total Files\"], axis=0)\n",
    "    return percentage_table * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_model = create_count_table(\"model\", \"attempt == 1 and template == 'controlled_md' and model != 'dolphincoder-starcoder2-15b'\")\n",
    "count_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not interesting\n",
    "# count_per_model.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "pd.options.plotting.backend = \"matplotlib\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"thesis_style_sheet_big.mplstyle\")\n",
    "#plt.style.use(\"default\")\n",
    "\n",
    "def make_bar_plot(graph_data: pd.DataFrame, percentage=True, title: str | None = None, xlabel=None, ylabel=None, bar_labels=False, xrot=0, ylim=100, label_col=5):\n",
    "\n",
    "    ax = graph_data.plot(kind='bar', title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    ax.legend(bbox_to_anchor=(0, 1.02, 1, 0.2), loc=\"lower left\",\n",
    "                mode=\"expand\", borderaxespad=0, ncol=label_col)\n",
    "    ax.tick_params(axis='x', labelrotation=xrot)\n",
    "    if percentage:\n",
    "        ax.set_ylim([0, ylim])\n",
    "        ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "    if bar_labels:\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_per_model = create_percentage_table(count_per_model).rename(index={\"codestral\": \"Codestral\", \"dolphin-2.6-mistral\": \"D-Mistral\", \"dolphin-2.6-phi-2\": \"D-Phi-2\", \"dolphin-2.7-mixtral\": \"D-Mixtral\", \"llama3-8b\": \"Llama 3\", \"phi3\": \"Phi-3\", \"mistral\": \"Mistral\", \"controlled_md\": \"MD\", \"via_description\": \"VT\", \"mixtral\": \"Mixtral\", \"controlled\": \"RM\"})\n",
    "# percentage_per_model.transpose().plot(kind=\"pie\", subplots=True, figsize=(40, 20), legend=True)\n",
    "make_bar_plot(percentage_per_model.filter([col for col in pp_steps + pp_errors if col not in ['MARKDOWN_CODEBLOCKS', 'MISSING_MD_START']]), xrot=90, ylim=None, label_col=3, xlabel=\"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_per_model.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_table[pp_steps + pp_errors].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_template = create_count_table(\"template\")\n",
    "count_per_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_per_template = create_percentage_table(count_per_template)\n",
    "percentage_per_template#.plot(kind=\"pie\", subplots=True, figsize=(40, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_template = create_count_table(\"template\", filter_query=\"attempt == 1\")\n",
    "percentage_per_template = create_percentage_table(count_per_template)\n",
    "percentage_per_template#.plot(kind=\"pie\", subplots=True, figsize=(40, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_target_pl = create_count_table(\"target_pl\")\n",
    "percentage_per_target_pl = create_percentage_table(count_per_target_pl)\n",
    "count_per_target_pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per model: per Template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_model_template = create_count_table([\"model\", \"template\"], filter_query=\"attempt == 1\")\n",
    "percentage_model_template = create_percentage_table(count_model_template)\n",
    "count_model_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_model_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per model: Per target language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_table[[\"model\", \"target_pl\"] + pp_steps + pp_errors].groupby([\"model\", \"target_pl\"]).count()\n",
    "\n",
    "count_model_target_pl = create_count_table([\"model\", \"target_pl\"])\n",
    "percentage_model_target_pl = create_percentage_table(count_model_target_pl)\n",
    "count_model_target_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_model_target_pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Per template: per model \n",
    "This is the same information as Per model: per template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_attempt = create_count_table(\"attempt\")\n",
    "count_per_attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_per_attempt = create_percentage_table(count_per_attempt)\n",
    "percentage_per_attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_percentage_table(create_count_table([\"model\", \"attempt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "pp_reports_dir = config.postprocessing_reports_dir\n",
    "\n",
    "report_files = list_json_files(pp_reports_dir)\n",
    "print(report_files)\n",
    "\n",
    "for report_f in report_files:\n",
    "    # postprocessing_reports/engine_model_template_attempt/jsonfilename\n",
    "    run_id = report_f.split(os.path.sep)[-2]\n",
    "    run_info = run_id.split(\"_\")\n",
    "    if len(run_info) == 3:\n",
    "        continue\n",
    "    engine = run_info[0]\n",
    "    model = run_info[1]\n",
    "    template = \"_\".join(run_info[2:-1])\n",
    "    attempt = int(run_info[-1])\n",
    "    # directory/pp_report_dataset.json\n",
    "    dataset = os.path.splitext(os.path.basename(report_f))[0].split(\"_\")[-1]\n",
    "\n",
    "    print(engine, model, template, attempt, dataset) \n",
    "\n",
    "    with open(report_f, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    num_files = len(data)\n",
    "    pp_steps_statistic = dict()\n",
    "    pp_errors_statistic = dict()\n",
    "    num_errors = 0\n",
    "    total_steps = 0\n",
    "    for file in data:\n",
    "        num_errors += len(data[file]['pp_errors'])\n",
    "        total_steps += len(data[file]['pp_steps'])\n",
    "        for step in data[file]['pp_steps']:\n",
    "            if step not in pp_steps_statistic.keys():\n",
    "                pp_steps_statistic[step] = 1\n",
    "            else:\n",
    "                pp_steps_statistic[step] += 1\n",
    "\n",
    "            # if step in [\"NATURAL_TEXT\", \"NO_MARKDOWN\", \"CODE_HEURISTIC\"]:\n",
    "            #     print(step, file)\n",
    "        \n",
    "        for error in data[file]['pp_errors']:\n",
    "            if error not in pp_errors_statistic.keys():\n",
    "                pp_errors_statistic[error] = 1\n",
    "            else:\n",
    "                pp_errors_statistic[error] += 1\n",
    "            # print(error, file)\n",
    "            \n",
    "    print(\"Total files:\", num_files)\n",
    "    print(\"Total errors: \", num_errors)\n",
    "    print(\"Total steps:\", total_steps)\n",
    "\n",
    "    for error, count in pp_errors_statistic.items():\n",
    "        print(error, count)\n",
    "\n",
    "    for step, count in pp_steps_statistic.items():\n",
    "        print(step, count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
